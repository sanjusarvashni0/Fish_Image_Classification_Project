{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcaae6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNet, InceptionV3, EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458728e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6225 images belonging to 11 classes.\n",
      "Found 1092 images belonging to 11 classes.\n",
      "Found 3187 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"../Dataset_fish_extracted/train\"\n",
    "val_dir = \"../Dataset_fish_extracted/val\"\n",
    "test_dir = \"../Dataset_fish_extracted/test\"\n",
    "\n",
    "#Training generator with augmentation\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Rescaling the val and test data\n",
    "val_test_gen = ImageDataGenerator(rescale=1./255)\n",
    "# Create Generators\n",
    "train_generator = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_gen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90b2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the class index as json file\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "class_names = [name for name in class_names if os.path.isdir(os.path.join(train_dir, name))]\n",
    "labels = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "with open('class_labels.json', 'w') as f:\n",
    "    json.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ec72b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### CUSTOM MADE CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbed1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sanju\\Fish_Image_Classification_Project\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a78ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d73e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 3s/step - accuracy: 0.4622 - loss: 1.5798 - val_accuracy: 0.1740 - val_loss: 7.5451\n",
      "Epoch 2/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 3s/step - accuracy: 0.5899 - loss: 1.2114 - val_accuracy: 0.5000 - val_loss: 2.0230\n",
      "Epoch 3/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 3s/step - accuracy: 0.6602 - loss: 1.0032 - val_accuracy: 0.8288 - val_loss: 0.6153\n",
      "Epoch 4/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 3s/step - accuracy: 0.6969 - loss: 0.8941 - val_accuracy: 0.8892 - val_loss: 0.4091\n",
      "Epoch 5/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 3s/step - accuracy: 0.7399 - loss: 0.7705 - val_accuracy: 0.8718 - val_loss: 0.4497\n",
      "Epoch 6/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 3s/step - accuracy: 0.7754 - loss: 0.6698 - val_accuracy: 0.8773 - val_loss: 0.4226\n",
      "Epoch 7/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 3s/step - accuracy: 0.7863 - loss: 0.6418 - val_accuracy: 0.8901 - val_loss: 0.3733\n",
      "Epoch 8/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 3s/step - accuracy: 0.8173 - loss: 0.5726 - val_accuracy: 0.9469 - val_loss: 0.2241\n",
      "Epoch 9/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 3s/step - accuracy: 0.8299 - loss: 0.5161 - val_accuracy: 0.8736 - val_loss: 0.4661\n",
      "Epoch 10/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 3s/step - accuracy: 0.8476 - loss: 0.4691 - val_accuracy: 0.8819 - val_loss: 0.4225\n",
      "Epoch 11/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 3s/step - accuracy: 0.8580 - loss: 0.4254 - val_accuracy: 0.9432 - val_loss: 0.2512\n",
      "Epoch 12/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 3s/step - accuracy: 0.8564 - loss: 0.4387 - val_accuracy: 0.9560 - val_loss: 0.2009\n",
      "Epoch 13/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 3s/step - accuracy: 0.8744 - loss: 0.3883 - val_accuracy: 0.9222 - val_loss: 0.3119\n",
      "Epoch 14/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 3s/step - accuracy: 0.8818 - loss: 0.3504 - val_accuracy: 0.9698 - val_loss: 0.1665\n",
      "Epoch 15/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 2s/step - accuracy: 0.8887 - loss: 0.3632 - val_accuracy: 0.9542 - val_loss: 0.1758\n",
      "Epoch 16/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 2s/step - accuracy: 0.8922 - loss: 0.3318 - val_accuracy: 0.9652 - val_loss: 0.1780\n",
      "Epoch 17/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 2s/step - accuracy: 0.8977 - loss: 0.3163 - val_accuracy: 0.8837 - val_loss: 0.4768\n",
      "Epoch 18/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - accuracy: 0.8998 - loss: 0.3239 - val_accuracy: 0.9231 - val_loss: 0.3260\n",
      "Epoch 19/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 2s/step - accuracy: 0.9033 - loss: 0.2998 - val_accuracy: 0.9744 - val_loss: 0.1318\n",
      "Epoch 20/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.9094 - loss: 0.2741 - val_accuracy: 0.9460 - val_loss: 0.2083\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7e5588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#Saving the trained model \n",
    "model.save('CustomCNN/custom_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ebf8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as keras\n",
    "model.save('CustomCNN/custom_cnn.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dd016",
   "metadata": {},
   "source": [
    "## Working on Pre Trained CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb1775",
   "metadata": {},
   "source": [
    "### Model 1: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
