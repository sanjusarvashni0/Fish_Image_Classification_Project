{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcaae6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNet, InceptionV3, EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458728e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6225 images belonging to 11 classes.\n",
      "Found 1092 images belonging to 11 classes.\n",
      "Found 3187 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"../Dataset_fish_extracted/train\"\n",
    "val_dir = \"../Dataset_fish_extracted/val\"\n",
    "test_dir = \"../Dataset_fish_extracted/test\"\n",
    "\n",
    "#Training generator with augmentation\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Rescaling the val and test data\n",
    "val_test_gen = ImageDataGenerator(rescale=1./255)\n",
    "# Create Generators\n",
    "train_generator = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_gen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90b2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the class index as json file\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "class_names = [name for name in class_names if os.path.isdir(os.path.join(train_dir, name))]\n",
    "labels = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "with open('class_labels.json', 'w') as f:\n",
    "    json.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ec72b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### CUSTOM MADE CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbed1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sanju\\Fish_Image_Classification_Project\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a78ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d73e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 3s/step - accuracy: 0.4622 - loss: 1.5798 - val_accuracy: 0.1740 - val_loss: 7.5451\n",
      "Epoch 2/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 3s/step - accuracy: 0.5899 - loss: 1.2114 - val_accuracy: 0.5000 - val_loss: 2.0230\n",
      "Epoch 3/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 3s/step - accuracy: 0.6602 - loss: 1.0032 - val_accuracy: 0.8288 - val_loss: 0.6153\n",
      "Epoch 4/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 3s/step - accuracy: 0.6969 - loss: 0.8941 - val_accuracy: 0.8892 - val_loss: 0.4091\n",
      "Epoch 5/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 3s/step - accuracy: 0.7399 - loss: 0.7705 - val_accuracy: 0.8718 - val_loss: 0.4497\n",
      "Epoch 6/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 3s/step - accuracy: 0.7754 - loss: 0.6698 - val_accuracy: 0.8773 - val_loss: 0.4226\n",
      "Epoch 7/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 3s/step - accuracy: 0.7863 - loss: 0.6418 - val_accuracy: 0.8901 - val_loss: 0.3733\n",
      "Epoch 8/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 3s/step - accuracy: 0.8173 - loss: 0.5726 - val_accuracy: 0.9469 - val_loss: 0.2241\n",
      "Epoch 9/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 3s/step - accuracy: 0.8299 - loss: 0.5161 - val_accuracy: 0.8736 - val_loss: 0.4661\n",
      "Epoch 10/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 3s/step - accuracy: 0.8476 - loss: 0.4691 - val_accuracy: 0.8819 - val_loss: 0.4225\n",
      "Epoch 11/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 3s/step - accuracy: 0.8580 - loss: 0.4254 - val_accuracy: 0.9432 - val_loss: 0.2512\n",
      "Epoch 12/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 3s/step - accuracy: 0.8564 - loss: 0.4387 - val_accuracy: 0.9560 - val_loss: 0.2009\n",
      "Epoch 13/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 3s/step - accuracy: 0.8744 - loss: 0.3883 - val_accuracy: 0.9222 - val_loss: 0.3119\n",
      "Epoch 14/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 3s/step - accuracy: 0.8818 - loss: 0.3504 - val_accuracy: 0.9698 - val_loss: 0.1665\n",
      "Epoch 15/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 2s/step - accuracy: 0.8887 - loss: 0.3632 - val_accuracy: 0.9542 - val_loss: 0.1758\n",
      "Epoch 16/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 2s/step - accuracy: 0.8922 - loss: 0.3318 - val_accuracy: 0.9652 - val_loss: 0.1780\n",
      "Epoch 17/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 2s/step - accuracy: 0.8977 - loss: 0.3163 - val_accuracy: 0.8837 - val_loss: 0.4768\n",
      "Epoch 18/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - accuracy: 0.8998 - loss: 0.3239 - val_accuracy: 0.9231 - val_loss: 0.3260\n",
      "Epoch 19/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 2s/step - accuracy: 0.9033 - loss: 0.2998 - val_accuracy: 0.9744 - val_loss: 0.1318\n",
      "Epoch 20/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.9094 - loss: 0.2741 - val_accuracy: 0.9460 - val_loss: 0.2083\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7e5588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#Saving the trained model \n",
    "model.save('CustomCNN/custom_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ebf8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as keras\n",
    "model.save('CustomCNN/custom_cnn.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dd016",
   "metadata": {},
   "source": [
    "## Working on Pre Trained CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb1775",
   "metadata": {},
   "source": [
    "### Model 1: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4106d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained VGG16\n",
    "\n",
    "VGG_base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93cdee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the output layer\n",
    "\n",
    "x = VGG_base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "model_vgg = Model(inputs=VGG_base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25c2045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the whole model as trainable\n",
    "VGG_base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last 4\n",
    "for layer in VGG_base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model_vgg.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "123d410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2373s\u001b[0m 12s/step - accuracy: 0.3133 - loss: 2.0011 - val_accuracy: 0.7079 - val_loss: 1.3619\n",
      "Epoch 2/5\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2698s\u001b[0m 14s/step - accuracy: 0.6072 - loss: 1.1959 - val_accuracy: 0.8214 - val_loss: 0.7490\n",
      "Epoch 3/5\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2559s\u001b[0m 13s/step - accuracy: 0.7414 - loss: 0.8234 - val_accuracy: 0.9203 - val_loss: 0.4379\n",
      "Epoch 4/5\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2231s\u001b[0m 11s/step - accuracy: 0.8180 - loss: 0.5917 - val_accuracy: 0.9460 - val_loss: 0.3113\n",
      "Epoch 5/5\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1839s\u001b[0m 9s/step - accuracy: 0.8654 - loss: 0.4507 - val_accuracy: 0.9570 - val_loss: 0.2385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the fine tuned VGG16 model\n",
    "\n",
    "history_vgg_finetune = model_vgg.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7af04e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Saving the model as h5\n",
    "model_vgg.save('VGG16/vgg_finetuned_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7edcac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as keras\n",
    "model_vgg.save('VGG16/vgg_finetuned_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d880621",
   "metadata": {},
   "source": [
    "### \n",
    "Model 2: ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f72013c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `weights` argument should be either `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#creating the base model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ResNet50_base_model = \u001b[43mResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sanju\\Fish_Image_Classification_Project\\venv\\Lib\\site-packages\\keras\\src\\applications\\resnet.py:409\u001b[39m, in \u001b[36mResNet50\u001b[39m\u001b[34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[39m\n\u001b[32m    406\u001b[39m     x = stack_residual_blocks_v1(x, \u001b[32m256\u001b[39m, \u001b[32m6\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33mconv4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stack_residual_blocks_v1(x, \u001b[32m512\u001b[39m, \u001b[32m3\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33mconv5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstack_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreact\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sanju\\Fish_Image_Classification_Project\\venv\\Lib\\site-packages\\keras\\src\\applications\\resnet.py:111\u001b[39m, in \u001b[36mResNet\u001b[39m\u001b[34m(stack_fn, preact, use_bias, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name, weights_name)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03m    A Model instance.\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (weights \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;129;01mor\u001b[39;00m file_utils.exists(weights)):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `weights` argument should be either \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`None` (random initialization), \u001b[39m\u001b[33m'\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(pre-training on ImageNet), \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor the path to the weights file to be loaded.  Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mweights=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights == \u001b[33m\"\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m include_top \u001b[38;5;129;01mand\u001b[39;00m classes != \u001b[32m1000\u001b[39m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf using `weights=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` with `include_top=True`, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`classes` should be 1000.  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived classes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The `weights` argument should be either `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights="
     ]
    }
   ],
   "source": [
    "#creating the base model\n",
    "ResNet50_base_model = ResNet50(weights = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594dfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
